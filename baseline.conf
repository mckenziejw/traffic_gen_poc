set system host-name ex1-1002
set system root-authentication encrypted-password "$6$isP7Xymk$hMk8/w3e6Ca91eyLBSHqn1rxxJ/XZLg8j1b8Ku8w5VEJ9AygvcT/JAZc/1Z9PCH4mJg4CyRx.dKyV4XWBMoCE."
set system commit synchronize
set system scripts language python3
set system scripts synchronize
set system services ssh root-login allow
set system services ssh protocol-version v2
set system authentication-order password
set system name-server 8.8.8.8
set system syslog file messages any notice
set system syslog file messages authorization any
set system syslog file messages archive size 2m
set system syslog file messages archive files 5
set system syslog file snapshot archive size 2m
set system syslog file snapshot archive files 5
set interfaces ge-0/0/0 unit 0 description pc-1002.ajmadev.wa
set interfaces ge-0/0/0 unit 0 family ethernet-switching vlan members AJMADEV_MGMT_K2
set interfaces ge-0/0/1 unit 0 description ap-1002.ajmadev.wa
set interfaces ge-0/0/1 unit 0 family ethernet-switching vlan members AJMADEV_MGMT_K2
set interfaces ge-0/0/23 unit 0 description xe-0-0-1.as.ajmadev.wa
set interfaces ge-0/0/23 unit 0 family ethernet-switching interface-mode trunk
set interfaces ge-0/0/23 unit 0 family ethernet-switching vlan members AJMADEV_MGMT_K2
set interfaces ge-0/0/23 unit 0 family ethernet-switching vlan members AJMADEV_VM1_K2
set interfaces ge-0/0/23 unit 0 family ethernet-switching vlan members AJMADEV_VM2_K2
set interfaces me0 unit 0 family inet address 10.210.14.10/29
set forwarding-options storm-control-profiles default all
set routing-options static route 0.0.0.0/0 next-hop 10.210.14.14
set protocols lldp interface all
set protocols lldp-med interface all
set protocols igmp-snooping vlan default
set poe interface all
set vlans AJMADEV_MGMT_K2 vlan-id 1661
set vlans AJMADEV_VM1_K2 vlan-id 802
set vlans AJMADEV_VM2_K2 vlan-id 803
![Slide1](M2/Slide1.PNG)

###### h6

![](M2/Slide2.png)

## Objectives

The slide lists the objectives for this module. We will begin with the first objective listed.

###### h6

![](M2/Slide3.png)

## Machine Learning 

We should not underestimate the future impact of machine intelligence on our businesses and societies. Recent centuries have seen many key technological innovations, from the industrial revolution to the space race, from the personal computer to the Internet. These technologies have displayed exponential growth in their adoption and impact. On an exponential growth curve, change occurs relatively slowly at first, but reaches an inflection point at which the rate of change explodes beyond normal human expectation. Let's look at the chronology of the Internet as an example:

*1965*: Two computers at MITs Lincoln Lab communicate with rudimentary packet switching technology

*1969*: The first primitive WAN is created between UCLA, UC Santa Barbara, the Stanford Research Institute and University of Utah. When a student attempted to login to the system, it crashed after the first two characters. That’s certainly primitive!

*1971*: ARPANET, a project of the US Department of Defense, is declared operational, interconnecting regional academic and military networks in the US.

*1972*: The first network mail protocol is introduced. 

*1973*: The first international network is created as the University College of London and Royal Radar Establishment in Norway connect to ARPANET.

*1974*: The first ISP is born with the introduction of a commercial version of ARPANET called Telenet

*1979*: USENET is formed and hosts the first news and discussion boards. At this point, only a very few of the very tech-savvy (read: super nerds) could use this early version of the Internet. 

*1983*: The TCP/IP protocol standard is implemented on ARPANET. DNS is introduced.

*1987*: The number of hosts on the Internet exceeds 20,000

*1991*: CERN introduces the world wide web to the public, thanks to HTML, developed there the previous year.

*1993*: The number of websites reaches 600.

*2000*: From 20,000 hosts a mere 13 years before, the Internet has grown to 390 million users

*2009*: The Internet reaches 1.9 billion users

*2014*: The Internet reaches 3 billion users, 44 percent of the world's population

*2018*: That number has reached 4.3 billion

As of 2022, it is estimated that 5.4 billion people now use the Internet. Consider this growth trajectory. It took 18 years for the Internet to grow to 20,000 hosts. Another 17 years and it had 390 million users. Fast forward another 18 years and that number has swelled to nearly half the population of the planet. This is what exponential growth looks like. 

We are still in the earliest stages of artificial intelligence (AI) and machine learning applications. These technologies have been largely the domain of academics and the very tech savvy for the last decade or more, but today it would be hard to find one Internet-connected person who has not interacted with AI. What will the world look like 20 years from now?

The quote above is from Nick Bostrom's book *Superintelligence: Paths, Dangers, Strategies*. Nick Bostrom is a Swedish philosopher who studies existential risk and the anthropic principle.

###### h6

![](M2/Slide4.png)

## What are AI and AIOps? Decoding the Jargon.

We'll deal with AIOps in the next module, so for now, let's attempt to deal with the barrage of new terminology the AI revolution has thrust into our lives. As with any new technology, there's a mixture of terms of art and marketing buzzwords we must untangle.

###### h6

![](M2/Slide5.png)

## Artificial Intelligence, Part 1—The Future

The phrase "Artificial Intelligence" can call many different images to our minds. You might picture SIRI, HAL, the Terminator, or ChatGPT. These are all valid examples, but none fully capture AI. Artificial Intelligence is an umbrella term encompassing the tools and techniques of a range of scientific and mathematical disciplines, but we can generally classify it into three categories:

***Artificial General Intelligence* (AGI)**

Artificial General Intelligence is one popular conception of AI. AGI systems are machines possessing human-like intelligence. Such systems could pass the famous Turing test. AI pioneer Alan Turing proposed the Turing test as a thought experiment to answer the question: how we would determine that a machine has reached human level intelligence? If a human could talk to the machine and not distinguish it from a human, it would pass the Turing test.

***Artificial Super Intelligence* (ASI)**

This is presently the stuff of movies and SciFi novels. ASI describes systems with super-human intelligence. This is not measured by sheer processing power, because machines surpassed humans long ago on that front. Rather, ASI describes machines that can replicate the creativity and insight of the human brain, coupled with vastly greater processing power. 

###### h6

![](M2/Slide6.png)

## Artificial Intelligence, Part 2—The Present

(Continued)

***Artificial Narrow Intelligence* (ANI)**

ANI is AI designed to accomplish a specific (narrow) task or set of tasks. All current, practical implementations of AI fall into this category, including Large Language Models, chatbots and Generative AI.

###### h6

![](M2/Slide7.png)

## Data Science

Data Science is a critical field of applied mathematics and provides many of the tools in the machine learning toolkit. Training machine learning models requires immense and coherent datasets for training. Data Science forms the basis not only for properly cleaning and analysing data, but also provides the algorithmic implementations for training machine intelligence systems.

###### h6

![](M2/Slide8.png)

## Machine Learning 

Machine Learning is a subfield of AI. In industry and in common usage, when we say “AI”, we really mean “Machine Learning”. Machine Learning is the process of building models that learn to make decisions or generate output implicitly by some form of trial and error. Note that this is fundamentally different from traditional computer programming, in which our software makes decisions based on a programmed set of rules. In fact, once an ML model has been trained, it is often difficult or impossible to reverse engineer its decisions into a set of rules. Machine learning models include:

- Neural networks

- Regression models

- Clustering models

- Decision trees

- Random forests

We will focus on neural networks as the illustrative example.

###### h6

![](M2/Slide9.png)

## Deep Learning

So we've discussed Machine Learning, but what is Deep Learning? Deep Learning is a subfield of ML and a term of art referring specifically to the details of a neural network implementation. We'll discuss the structure of neural nets in more detail later, but "Deep" in this context refers to the complexity (depth) of a neural network. A neural network with more than 3 layers is generally called a "Deep" neural network.

One important characteristic of Deep Learning models is their ability to be trained on and generate unstructured data such as text, images, video, and audio. In fact, Deep Learning underpins most of the mainstream AI tools.

###### h6

![](M2/Slide10.png)

## AI and Machine Learning—The Big Picture

This diagram recaps the relationships between Artificial Intelligence, Machine Learning, Data Science and Deep Learning we discussed in this section.

###### h6

![](M2/Slide11.png)

## Objectives

The slide lists the objectives for this module.

###### h6

![](M2/Slide12.png)

## Machine Learning Categories

Having dispensed with some of the jargon, let's dig deeper on Machine Learning. Naturally, we must deal with more jargon at this point!

While Machine Learning is an immense field, ML models can be broadly classified based on the methods used to train them. The lists above include many specific examples, but we will not be going into them in detail. Instead we want to get the big picture before we begin our treatment of Neural Networks.

**Supervised Learning**  

These models are trained on a dataset where the correct outputs are known in advance. We tune the model based on how closely its outputs conform to the correct answer. This is useful for systems designed to accomplish a specific purpose, as we'll see in the next section.

**Unsupervised Learning**

We train these models on data where the "right" answers are not known in advance. These types of models can be used for advanced pattern recognition in large datasets. Whereas with supervised learning we are seeking specific correct outputs, unsupervised models enable us to generate new insights by identifying patterns and correlations that human analysts may not identify. Hidden clusters and hidden correlations are the domain of unsupervised learning and can help researchers with hypothesis generation.

***Reinforcement Learning***

This is a variant of Supervised Learning. The key difference is that reinforcement learning models are trained with real-time data, rather than canned sets of training data. These models operate in live environments where every time they produce an output, we as trainers "reward" or "punish" them based on how much we like the output. The system adjusts itself over time to make us happy!

###### h6

![](M2/Slide13.png)

## Machine Learning Types and Applications

This diagrams lists some of the common applications for different types of machine learning. In practice, many systems combine multiple models from multiple categories, choosing the right tool for each job within the larger project.

###### h6

![](M2/Slide14.png)

## Machine Learning Methods-Supervised Learning

Supervised Learning is probably the most straightforward ML variant to understand, so it is a good place to start! With Supervised Learning, we as the ML engineers have a specific task that we want to offload onto the system. Image classification is the classic example. In this case we start with a set of training data labelled with correct classifications and attempt to train the system to correctly classify new images. There are three components to the Supervised Learning algorithm:

**Decision Process**:

The model attempts to classify whatever data we give it as input.

**Error Function**:

The error function determines how close the system came to returning the correct answer.

**Optimization Process**:

Once we've determined how close the system came to a correct answer, this optimization process adjusts the internal parameters of the system. These adjustments gradually increase the odds of generating a more correct answer next time. 

These three components create an iterative process. We take thousands or millions of training inputs (training data) and run each one through the system. The model is tuned after every cycle until it produces correct responses with high reliability.

###### h6

![](M2/Slide15.png)

## Hot Dog or Not Hot Dog?

You have been tasked with developing a model to process images and answer the all-important question: Hot Dog or Not Hog Dog? What exactly does this look like?

This is an appropriate application for Supervised Learning. It is easy to determine if the model produces correct outputs (every image is definitively either a hot dog or not), so we can develop a clear error function. 

So we first need a training dataset. This will just be a large set of images, including both pictures of hot dogs and other things that are not hot dogs. We might choose to use only food images, or a broader image set, including lots of other non-food objects. The quality of your dataset will ultimately determine the quality of your model. Remember, our model knows absolutely nothing except what we teach it. If this model is to be used in a food identification app, we might choose to use only food images and keep the model specialized. If our users are taking a picture of their cat, they shouldn't reasonably expect an accurate response. If this is part of a larger image classification suite, we might want more varied images in the dataset.

Once we have our training dataset, we can label all our images. This will allow us to automate the error function, rather than having to manually check each output. In the slide above we show an example labeled image.

Once we have our training dataset in order, we can begin the iterative process of Decision -> Error -> Optimization. In 2024, there are many standardized models for this type of training, but let's take a peek at what's happening under the hood.

###### h6

![](M2/Slide16.png)

## Neural Networks Explained

Neural Networks are essential in today's Machine Learning landscape. The term Neural Network reflects the logical structure of the model, which is meant to emulate the structure of neurons in the human brain.

In the brain, every neuron has numerous connections to other neighboring neurons. The strength of these connections ultimately determines the function of the system as a whole, and is being constantly adjusted based on new input. Let's say that you are trying to learn how to bowl. This is going to involve a specific part of your brain dedicated to movement and coordination. Every action you take, from dragging yourself out of bed in the morning to typing on your computer, is guided by electrical signals that traverse this system of neurons. Every signal in your brain is propagated between neurons based on the strength of their connections to each other neuron, ultimately leading to signals to your muscles causing them to move your body.

When you're learning to bowl and roll your first frame, your motions are just your best guess based on everything you've thrown in the past. It's probably not going to be very good. When you inevitably throw a gutter ball, your brain is going to fire off neurotransmitters associated with stress and negative emotions. These are essentially your error function. This tells the active region of your brain that your neurons weren't firing along the right path. Consequently, all of the connections that fired to make you throw that horrible gutter ball are going to be slightly weakened. If you manage to hit some pins the next time, a good jolt of feel-good dopamine will signal the opposite, and all the connections that fired to score those pins will be strengthened. With every throw the strength of these neural connections will be adjusted as you get better and better.

A neural network is a simplified version of what's happening in your brain. We feed our input data into one end of the network, assigning one chunk of that data to each node (neuron) on the input side. Each of these nodes has a connection to every node in the next layer of the system. When we first start, those connections all have random strengths. 

Each of these middle nodes also has a connection to the output node. If the signal received from the inputs is strong enough, each node will pass on its signal to the output node. This is the **feed forward** process. The signal strength required to pass on this signal is called the ***threshold*** and is measured by a ***threshold function***.

Those output nodes also have their own threshold function, and this is what makes the final decision. This is where the system decides if the image is a hot dog or not. If the signal is strong enough, it outputs "HOTDOG". Otherwise it outputs "NOT HOT DOG".

Finally, the output is checked against the input. If the output was correct, then the connections that fired to produce that output are strengthened so they will fire with more strength the next time. Conversely, the connections that didn't fire might be weakened so they will be even less likely to fire. This is **back propagation**.

Note that in this example, there is only one output node because we are making a binary decision. If there were multiple possible outputs, these would require more output nodes.

###### h6

![](M2/Slide17.png)

## Hot Dog or Hamburger Revisited—Big Picture

Let's walk through the process in more detail. Here we are using 64x64 pixel input images. The most straightforward way to break up this input data is to treat each pixel and its associated color value as one unit of input. We then need one input node for each pixel, so in this example 4096 input nodes. For the sake of simplicity, we have converted our images to monochrome, so each pixel has a value of either 0 (white) or 1 (black).

We then need a middle layer of nodes, called the **hidden layer**. Each input node has a connection to every hidden layer node. When we first start training, these connections are assigned random **weights** which define the strength of the connection. Each hidden node decides if the total strength of all incoming signals is strong enough to be passed on. If it is, then the signal is relayed to the output node. Just as with the input -> hidden connections, these connections also start with randomly assigned weights. 

Finally, the output node adds up all the received signals and decides if the signal is strong enough to output "HOTDOG".

###### h6

![](M2/Slide18.png)

## What’s in a Node? Part 1

As mentioned in the previous slide, every connection has a **weight** which  tells the communicating nodes how much value to assign to its signal. The weight could be an arbitrary number, but is often expressed on a relative scale between 0 and 1, meaning 0 to 100%.

###### h6

![](M2/Slide19.png)

## What’s in a Node? Part 2

Interesting things really start to happen at the hidden layer. Recall that every hidden layer node will potentially receive a signal from every input layer node. This is a lot of information! To decide what to do with all that information, the node first sums up all the signals received from the input nodes. But this is not just a simple sum, because it must account for the weight of each connection, so a **weighted sum** is used. Each input signal is multiplied by the weight of the connection, then all these values are added together.

Some models also allow the developer to set a bias, which is an additional fixed value that is added. This provides an additional parameter than can be adjusted during optimization.

###### h6

![](M2/Slide20.png)

## What’s in a Node? Part 3

Once the weighted sum is calculated, the node has a choice to make. It can either pass along a signal to the output node or not. This is determined by an **activation function** or a **threshold function**. There are many such functions, but for this example it will return a yes/no decision. If it returns yes (1), then the node will propagate its signal to the output.

Finally, the output node will run this same process again. It calculates the weighted sum of all received signals, runs its own activation function, and returns a result.

###### h6

![](M2/Slide21.png)

## Hot Dog or Hamburger Revisited—Feed Forward

The process starts with the input data on the input nodes, which is processed by the hidden and output nodes to return a result. This is called the **Feed Forward** process. 

###### h6

![](M2/Slide22.png)

## What’s Next? Back Propagation!

The learning (**optimization**) occurs during the **back propagation** process. 

After the neural network has returned an output, we use an **error function** to determine if the result was correct and, if not, how wrong it was. In our case it's quite simple because it's a binary result. It is either 100% right or 100% wrong.

In either case, the parameters of the model will be adjusted accordingly. If the result was correct, we will optimize the model by slightly increasing the weight of the paths that lead to returning the correct result, so they will be more likely to give us the same result next time. If the result was incorrect, we weaken those connections. 

###### h6

![](M2/Slide23.png)

## Back Propagation

This slide spells out what happened after our neural network returned a correct response:

1. We first determine that the output was correct

2. Since it was, we start back propagation from the output node. We retrace the connections that gave us a positive signal and strengthen their weights

3. Depending on the model, we may also weaken the connections that gave us the wrong signal

4. This process continues all the way back to the input nodes.

###### h6

![](M2/Slide24.png)

## What Next? We do it Again! And Again and Again…

So now that we've seen how one training image affects the model, what's next?

We do the same thing over and over again. Beural networks must be trained on large sets of training data until they start to produce reliably accurate results. It is also important to validate the model against a new set of data that it hasn't seen yet, to make sure that we haven't just trained the model to only work with the specific images in our data set.

There is far more left to understand about neural networks, and even more about machine learning in general, but this very simplified example should give us a good starting point to understand how these mysterious systems actually work.

###### h6

![](M2/Slide25.png)

## A Word of Caution

Hopefully, this was a fun introduction to machine learning, but we've barely scratched the surface. Our example is a very primitive neural network with a few of the finer details sanded away. In practice, thing can be quite a bit more complicated!

- ML models are often multi-stage processes composed of many layers. Neural networks are just one piece of the puzzle.

- Our example is a fully-connected neural network, but that is not always optimal. Sparse networks have connections only between a subset of nodes, while recurrent neural networks (RNNs) may have links that connect back to previous layers.

- Activation functions are not always applied at every node. Choosing which functions to use and where to apply them is an important part of design and development.

- While weights are generally initialized randomly, that randomness might follow a specific statistical distribution (normal, log-normal, student's-t, etc.)

- Simple feed-forward neural networks are only suitable for the simplest image classification tasks. Newer architectures like convolutional neural networks (CNNs) and deep residual networks (DRNs) are more common in real-world applications.

![](M2/Slide26.png)

## Summary

In this module we:

- Clarified AI and ML terminology

- Explained the basics of machine learning

![](M2/Slide27.png)

## Thank You!

